{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **前日の20:00までのデータを用いて翌日の00:00~23:30までの日射時間(sl)をMLPで予測する**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **必要な関数・ライブラリ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ加工・処理・分析モジュール\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_time(dataframe, col_name):\n",
    "    '''\n",
    "    to_datetimeを使うための前処理\n",
    "    '''\n",
    "    dataframe[col_name] = dataframe[col_name].map(lambda x : transform_time(x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_time(x):\n",
    "    '''\n",
    "    set_time内で使う関数\n",
    "    to_datetimeで24時をサポートしないので00に変更する処理\n",
    "    '''\n",
    "    str_x = str(x)\n",
    "    res = ''\n",
    "    if str(x)[8:10] == '24':\n",
    "        res = str_x[0:4] + '-' + str_x[4:6] + '-' + str_x[6:8] + ' 00:'+str_x[10:12] \n",
    "    else:\n",
    "        res = str_x[0:4] + '-' + str_x[4:6] + '-' + str_x[6:8] + ' '+ str_x[8:10] +':'+str_x[10:12]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_array(x):\n",
    "    '''\n",
    "    min, max, min-max正規化を行なった配列(np.array)を返す\n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    normalized = (x - x_min) / (x_max - x_min) \n",
    "    return x_min, x_max, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denormalize_array(normalized_x, x_min, x_max):\n",
    "    '''\n",
    "    正規化前のmin, maxを用いて元のスケールに戻す\n",
    "    '''\n",
    "    normalized_x = np.array(normalized_x)\n",
    "    denormalize_array = (normalized_x) * (x_max - x_min) + x_min\n",
    "    return denormalize_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_data(X):\n",
    "    '''\n",
    "    全時系列データから00:00 ~ 20:00までのデータごとに分割する\n",
    "    '''\n",
    "    input_list = []\n",
    "    # 一日は48個の時間帯に分けられる\n",
    "    total_size = len(X) // 48\n",
    "    reshaped_size = X[0:41].shape[0] * X[0:41].shape[1]\n",
    "    for i in range(total_size+1):\n",
    "        each_data = X[(i*48):(i*48+41)]\n",
    "        reshaped = each_data.reshape(reshaped_size)\n",
    "        input_list.append(list(reshaped))\n",
    "    input_list = np.array(input_list)\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_output_data(Y):\n",
    "    '''\n",
    "    全時系列データから00:00 ~ 23:30までのデータごとに分割する\n",
    "    '''\n",
    "    output_list = []\n",
    "    # 一日は48個の時間帯に分けられる\n",
    "    total_size = len(Y) // 48\n",
    "    for i in range(total_size):\n",
    "        each_data = Y[(i*48):(i*48+48)]\n",
    "        reshaped = each_data.reshape(48)\n",
    "        output_list.append(list(reshaped))\n",
    "    output_list = np.array(output_list)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_nan(X, Y):\n",
    "    '''\n",
    "    正解データがnanであるデータの組を削除\n",
    "    '''\n",
    "    mask = np.isnan(Y)\n",
    "    X = X[~mask]\n",
    "    Y = Y[~mask]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mae(X, Y):\n",
    "    '''\n",
    "    X, Yがメモリを食いすぎるのでfor文で計算\n",
    "    '''\n",
    "    mse = 0\n",
    "    for i in range(len(X)):\n",
    "        mse += np.abs(X[i]- Y[i])\n",
    "    return mse/len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **データの準備**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測する発電所番号を決める\n",
    "target_place = 1\n",
    "model_name = \"mlp_\"+str(target_place)\n",
    "try:\n",
    "    os.mkdir('./data/'+model_name)\n",
    "except:\n",
    "    print(\"file exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# アメダスデータの読み込み\n",
    "if target_place == 1 or target_place == 2:\n",
    "    # 名古屋アメダスのデータを使って予測する, amd_51106\n",
    "    # 対象アメダスは横浜アメダス, amd_46106\n",
    "    # 各amdidはamd_masterに記載されている\n",
    "    amd_data = pd.read_csv('data/raw_data/amd_51106.tsv', delimiter = '\\t')\n",
    "    amd_data = set_time(amd_data, 'datetime')\n",
    "    amd_data['datetime'] = amd_data['datetime'].map(lambda x : pd.to_datetime(x))\n",
    "    target_amd_data = pd.read_csv('data/raw_data/amd_46106.tsv', delimiter = '\\t')\n",
    "    target_amd_data = set_time(target_amd_data, 'datetime')\n",
    "    target_amd_data['datetime'] = target_amd_data['datetime'].map(lambda x : pd.to_datetime(x))\n",
    "elif target_place == 3:\n",
    "    # 甲府アメダスのデータを使って予測する, amd_49142\n",
    "    # 各amdidはamd_masterに記載されている\n",
    "    amd_data = pd.read_csv('data/raw_data/amd_49142.tsv', delimiter = '\\t')\n",
    "    amd_data = set_time(amd_data, 'datetime')\n",
    "    amd_data['datetime'] = amd_data['datetime'].map(lambda x : pd.to_datetime(x))\n",
    "else:\n",
    "    raise ValueError(\"invalid input target_place_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr</th>\n",
       "      <th>max_ws</th>\n",
       "      <th>ave_ws</th>\n",
       "      <th>max_tp</th>\n",
       "      <th>min_tp</th>\n",
       "      <th>sl</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 01:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pr  max_ws  ave_ws  max_tp  min_tp   sl            datetime\n",
       "0   0    68.0    41.0    73.0    68.0  0.0 2012-01-01 00:00:00\n",
       "1   0    81.0    56.0    46.0    39.0  0.0 2012-01-01 00:30:00\n",
       "2   0    81.0    52.0    43.0    40.0  0.0 2012-01-01 01:00:00\n",
       "3   0    67.0    40.0    44.0    39.0  0.0 2012-01-01 01:30:00\n",
       "4   0    33.0    20.0    43.0    37.0  0.0 2012-01-01 02:00:00"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30分毎のデータに編集\n",
    "amd_data = amd_data[[\"datetime\", \"pr\", \"max_ws\", \"ave_ws\", \"max_tp\", \"min_tp\", \"sl\"]]\n",
    "\n",
    "# 欠損値を一つ前の値で置換/output_data\n",
    "amd_data = amd_data.fillna(method='bfill')\n",
    "amd_30 = amd_data.set_index('datetime').groupby(pd.TimeGrouper(freq='1800s', closed='left')).sum()\n",
    "\n",
    "# datetimeのカラムを復活させる\n",
    "amd_30['datetime'] = amd_30.index\n",
    "amd_30.index = np.arange(len(amd_30))\n",
    "amd_30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 01:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sl            datetime\n",
       "0  0.0 2012-01-01 00:00:00\n",
       "1  0.0 2012-01-01 00:30:00\n",
       "2  0.0 2012-01-01 01:00:00\n",
       "3  0.0 2012-01-01 01:30:00\n",
       "4  0.0 2012-01-01 02:00:00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30分毎のデータに編集\n",
    "target_amd_data = target_amd_data[[\"datetime\", \"sl\"]]\n",
    "\n",
    "# 欠損値を一つ前の値で置換/output_data\n",
    "target_amd_data = target_amd_data.fillna(method='bfill')\n",
    "target_30 = target_amd_data.set_index('datetime').groupby(pd.TimeGrouper(freq='1800s', closed='left')).sum()\n",
    "\n",
    "# datetimeのカラムを復活させる\n",
    "target_30['datetime'] = target_30.index\n",
    "target_30.index = np.arange(len(target_30))\n",
    "target_30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデル構築のためにデータを分割する\n",
    "\n",
    "# 学習に必要なデータ\n",
    "# 前日の00:00 ~ 20:00のデータで翌日の0:00 ~ 23:30を予測する\n",
    "train_x_s_idx = amd_30[amd_30['datetime'] == pd.to_datetime('2012-01-01 00:00')].index[0]\n",
    "train_x_e_idx = amd_30[amd_30['datetime'] == pd.to_datetime('2015-12-30 20:00')].index[0]\n",
    "train_y_s_idx = amd_30[amd_30['datetime'] == pd.to_datetime('2012-01-02 00:00')].index[0]\n",
    "train_y_e_idx = amd_30[amd_30['datetime'] == pd.to_datetime('2015-12-31 23:30')].index[0]\n",
    "\n",
    "output_tp = target_30[\"sl\"][train_y_s_idx:train_y_e_idx+1]\n",
    "input_data = amd_30[[ \"pr\", \"max_ws\", \"ave_ws\", \"max_tp\", \"min_tp\", \"sl\"]][train_x_s_idx:train_x_e_idx+1]\n",
    "\n",
    "# 予測に必要なデータ\n",
    "# 前日の00:00 ~ 20:00のデータで翌日の0:00 ~ 23:30を予測する\n",
    "# test_y_s_idx = amd_data[amd_data['datetime'] == pd.to_datetime('2015-12-31 00:00')].index[0]\n",
    "# test_y_e_idx = amd_data[amd_data['datetime'] == pd.to_datetime('2017-3-31 23:50')].index[0]\n",
    "# test_x_s_idx = test_y_s_idx - chunk_size\n",
    "# test_x_e_idx = amd_data[amd_data['datetime'] == pd.to_datetime('2017-3-31 23:40')].index[0]\n",
    "\n",
    "# test_input_tp = amd_data[test_x_s_idx:test_x_e_idx+1]\n",
    "# test_output_tp = amd_data[test_y_s_idx:test_y_e_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPに突っ込むための準備をする\n",
    "\n",
    "# rnnに突っ込むためにmin-max正規化しておく\n",
    "normalized_input = (input_data - input_data.min()) / (input_data.max() - input_data.min())\n",
    "normalized_input = np.array(normalized_input)\n",
    "normalized_output = (output_tp - output_tp.min()) / (output_tp.max() - output_tp.min())\n",
    "normalized_output = np.array(normalized_output)\n",
    "\n",
    "input_list = get_input_data(normalized_input)\n",
    "output_list = get_output_data(normalized_output)\n",
    "\n",
    "# testデータの入力を用意\n",
    "# normalized_test_input = (test_input_tp - test_input_tp.min()) / (test_input_tp.max() - test_input_tp.min())\n",
    "# normalized_test_input = np.array(normalized_test_input)\n",
    "# test_input_list = get_chunked_data(normalized_test_input, chunk_size)\n",
    "\n",
    "# denormalize用\n",
    "output_max = float(output_tp.max())\n",
    "output_min = float(output_tp.min())\n",
    "\n",
    "# RNNに突っ込むためにデータを整形\n",
    "X = np.array(input_list).reshape(input_list.shape[0], input_list.shape[1])\n",
    "Y = np.array(normalized_output).reshape(output_list.shape[0], output_list.shape[1])\n",
    "# X_predict = np.array(test_input_list).reshape(len(test_input_list), chunk_size, test_input_list.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 48)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 246)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **モデルの構築**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件:翌日の気温を予測するには前日の20:00までの気象情報しか使えない\n",
    "\n",
    "モデル:翌日の30分毎の気温を前日の20:00までの各所のアメダスの気象情報で予測させる\n",
    "\n",
    "入力:各所のアメダス情報の配列\n",
    "\n",
    "出力:0:00, 0:30, .... , 23:00, 23:30の気温の配列\n",
    "\n",
    "とりあえず一つの箇所のデータを使って予測\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今までLSTMやってたからとりあえずLSTMでやってみる\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, X, Y, epochs = 100, batch_size = 100, model_name = \"test_model\"):\n",
    "        # 学習データと検証用データに分けておく\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=int((X.shape[0] * 0.1)))\n",
    "        self.X = X # 入力\n",
    "        self.Y = Y # 教師\n",
    "        self.X_val = X_val # 検証用\n",
    "        self.Y_val = Y_val #検証用\n",
    "        \n",
    "        '''\n",
    "        諸変数の設定\n",
    "        '''\n",
    "        self.input_layer_size = self.X.shape[1] #入力層の数\n",
    "        self.hidden_layer_size = 100 # 隠れ層の数、適当\n",
    "        self.output_layer_size = self.Y.shape[1] #出力層の数\n",
    "        self.batch_size = batch_size #バッチサイズ\n",
    "        self.learning_rate = 0.01 # 学習率 適当\n",
    "        self.epochs = epochs #エポック数\n",
    "        \n",
    "        # 学習データの保存\n",
    "        self.model_name = str(model_name)\n",
    "        \n",
    "        \n",
    "    def shuffle(self):\n",
    "        '''\n",
    "        ミニバッチかする際にランダムにシャッフル\n",
    "        '''\n",
    "        idx = [i for i in range(self.X.shape[0])]\n",
    "        np.random.shuffle(idx)\n",
    "        xs = np.array([[y for y in list(self.X[r])] for r in idx])\n",
    "        ys = np.array([self.Y[r] for r in idx])\n",
    "        return xs, ys\n",
    "        \n",
    "    def inference(self, input_ph):\n",
    "        '''\n",
    "        グラフの構築\n",
    "        '''\n",
    "        \n",
    "        # 重みとバイアスの初期化\n",
    "        hidden_w = tf.Variable(tf.truncated_normal([self.input_layer_size, self.hidden_layer_size], stddev=0.01), name='hidden_w')\n",
    "        hidden_b = tf.Variable(tf.truncated_normal([self.hidden_layer_size]), name='hidden_b')\n",
    "        output_w = tf.Variable(tf.truncated_normal([self.hidden_layer_size, self.output_layer_size], stddev=0.01), name='output_w')\n",
    "        output_b = tf.Variable(tf.truncated_normal([self.output_layer_size]), name='output_b')\n",
    "        \n",
    "        # 計算\n",
    "        print(\"input : \", input_ph)\n",
    "        hidden = tf.sigmoid(tf.matmul(input_ph, hidden_w) + hidden_b)\n",
    "        print(\"hidden : \", hidden)\n",
    "        output = tf.sigmoid(tf.matmul(hidden, output_w) + output_b)\n",
    "        print(\"output : \", output)\n",
    "        \n",
    "        weights = [hidden_w, output_w, hidden_w, hidden_b]\n",
    "        \n",
    "        return output, weights\n",
    "        \n",
    "    def loss(self, output_ph, actual_ph):\n",
    "        '''\n",
    "        MSEを使用\n",
    "        '''\n",
    "        cost = tf.reduce_mean(tf.square((output_ph - actual_ph)))\n",
    "        tf.summary.scalar('loss', cost)\n",
    "        return cost\n",
    "    \n",
    "    def training(self, cost):\n",
    "        '''\n",
    "        adamを仕様beta1, beta2は元論文の推奨値を仕様\n",
    "        '''\n",
    "        print(\"here\")\n",
    "        with tf.name_scope(\"training\") as scope:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999).minimize(cost)\n",
    "            return optimizer\n",
    "    \n",
    "    def train(self):\n",
    "        '''\n",
    "        学習\n",
    "        '''\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        n_batch = self.X.shape[0] // self.batch_size\n",
    "        \n",
    "        with tf.Graph().as_default():\n",
    "            # 変数の用意\n",
    "            input_ph = tf.placeholder('float', [None, self.input_layer_size], name='input')\n",
    "            actual_ph = tf.placeholder('float', [None, self.output_layer_size], name='actual_value')\n",
    "\n",
    "            prediction, weights = self.inference(input_ph)\n",
    "            cost = self.loss(prediction, actual_ph)\n",
    "            optimizer = self.training(cost)\n",
    "\n",
    "            # TensorBoardで可視化する\n",
    "            summary = tf.summary.merge_all()\n",
    "            # 初期化\n",
    "            init = tf.global_variables_initializer()\n",
    "                \n",
    "            # ここから学習\n",
    "            with tf.Session() as sess:\n",
    "                # 学習したモデルも保存しておく\n",
    "                saver = tf.train.Saver()\n",
    "                summary_writer = tf.summary.FileWriter(\"/tmp/tensorflow_log\", graph=sess.graph)\n",
    "                sess.run(init)\n",
    "\n",
    "                for epoch in range(self.epochs):\n",
    "                    X_, Y_ = self.shuffle()\n",
    "                    for i in range(n_batch):\n",
    "                        start = i * self.batch_size\n",
    "                        end = start + self.batch_size\n",
    "                        inputs  = X_[start:end]\n",
    "                        actuals = Y_[start:end]\n",
    "                        train_dict = {\n",
    "                            input_ph:      inputs,\n",
    "                            actual_ph:     actuals,\n",
    "                        }\n",
    "                    \n",
    "                    sess.run(optimizer, feed_dict=train_dict)\n",
    "\n",
    "                    if (epoch) % (self.epochs//10) == 0:\n",
    "                        val_dict = {\n",
    "                            input_ph:      self.X_val,\n",
    "                            actual_ph:     self.Y_val,\n",
    "                        }\n",
    "                        summary_str, train_loss = sess.run([summary, cost], feed_dict=val_dict)\n",
    "                        print(\"train#%d, validation loss: %e\" % (epoch, train_loss))\n",
    "                        summary_writer.add_summary(summary_str, epoch)\n",
    "                        \n",
    "                    datas = sess.run(weights)\n",
    "                    saver.save(sess,  \"./data/\" + str(self.model_name) + \"/\" + str(self.model_name) + \".ckpt\")\n",
    "                    \n",
    "                datas = sess.run(weights)\n",
    "                saver.save(sess, \"./data/\" + str(self.model_name) + \"/\" + str(self.model_name) + \".ckpt\")\n",
    "                \n",
    "    def predict(self, X_predict, model_name = \"test_model\"):\n",
    "        '''\n",
    "        予測期間に該当するデータから予測\n",
    "        '''\n",
    "        # 予測に使う変数の用意\n",
    "        tf.reset_default_graph()\n",
    "        input_ph = tf.placeholder(\"float\", [None, self.chunk_size, self.input_layer_size], name='input')\n",
    "        prediction, weights = self.inference(input_ph)\n",
    "        pre_dict = {\n",
    "            input_ph: X_predict,\n",
    "        }\n",
    "        \n",
    "        # 初期化\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # 保存したモデルをロード\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess,  \"./data/\" + str(self.model_name) + \"/\" + str(self.model_name) + \".ckpt\")\n",
    "\n",
    "            # ロードしたモデルを使って予測結果を計算\n",
    "            expected_output = sess.run([prediction], feed_dict=pre_dict)\n",
    "\n",
    "\n",
    "        return expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model = MLP(X, Y, model_name = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  Tensor(\"input:0\", shape=(?, 246), dtype=float32)\n",
      "hidden :  Tensor(\"Sigmoid:0\", shape=(?, 100), dtype=float32)\n",
      "output :  Tensor(\"Sigmoid_1:0\", shape=(?, 48), dtype=float32)\n",
      "here\n",
      "train#0, validation loss: 2.099135e-01\n",
      "train#10, validation loss: 1.075154e-01\n",
      "train#20, validation loss: 1.065942e-01\n",
      "train#30, validation loss: 1.000316e-01\n",
      "train#40, validation loss: 9.780588e-02\n",
      "train#50, validation loss: 1.013508e-01\n",
      "train#60, validation loss: 9.752372e-02\n",
      "train#70, validation loss: 9.135237e-02\n",
      "train#80, validation loss: 8.894075e-02\n",
      "train#90, validation loss: 8.764825e-02\n"
     ]
    }
   ],
   "source": [
    "mlp_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
